{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "HF_TOKEN=os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGINGFACE_REPO_ID=\"mistralai/Mistral-7B-Instruct-v0.3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup LLM (Mistral with huggingface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llm(huggingface_repo_id):\n",
    "    llm=HuggingFaceEndpoint(\n",
    "        repo_id=huggingface_repo_id,\n",
    "        temperature=0.5,\n",
    "        model_kwargs={\"token\":HF_TOKEN,\n",
    "                      \"max_length\":512}\n",
    "    )\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect LLM With FAISS and Create chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create custom prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_PROMPT_TEMPLATE = \"\"\"\n",
    "Use the pieces of information provided in the context to answer user's question.\n",
    "If you dont know the answer, just say that you dont know, dont try to make up an answer.\n",
    "Dont provide anything out of the given context\n",
    "\n",
    "Context:{context}\n",
    "Question:{question}\n",
    "\n",
    "Start the answer directly. No small talk please.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_custom_prompt(custom_prompt_template):\n",
    "    promt=PromptTemplate(template=custom_prompt_template, input_variables=['context','question'])\n",
    "    return promt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_FAISS_PATH='vectorstore/db_faiss'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Deep\\.conda\\envs\\vitabot\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "embedding_model=HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "db=FAISS.load_local(DB_FAISS_PATH, embedding_model, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create QA Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "qa_chain=RetrievalQA.from_chain_type(\n",
    "    llm=load_llm(HUGGINGFACE_REPO_ID),\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=db.as_retriever(search_kwargs={'k':3}),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={'prompt':set_custom_prompt(CUSTOM_PROMPT_TEMPLATE)}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invoke with a single query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Deep\\.conda\\envs\\vitabot\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULT: \n",
      "Cancer can be cured through surgical removal of the tumor, which is usually the best chance for a surgical cure, particularly for the first operation. Chemotherapy may also be used in the treatment of certain cancers. Radiation may occasionally be used, particularly for tumors in the brain.\n",
      "Source Document: [Document(id='71c15823-4a39-4146-92d7-08ec62d7f4db', metadata={'producer': 'GPL Ghostscript 9.10', 'creator': '', 'creationdate': '2017-05-01T10:37:35-07:00', 'moddate': '2017-05-01T10:37:35-07:00', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'source': 'data\\\\The_GALE_ENCYCLOPEDIA_of_MEDICINE_SECOND (1).pdf', 'total_pages': 759, 'page': 26, 'page_label': '27'}, page_content='curative for some stomach, genital/urinary, thyroid,\\nbreast, skin, and central nervous system cancers. The best\\nchance for a surgical cure is usually with the first opera-\\nGALE ENCYCLOPEDIA OF MEDICINE 2638\\nCancer therapy, definitive'), Document(id='081e0d1a-36eb-4320-b411-0f01b3e9db6e', metadata={'producer': 'GPL Ghostscript 9.10', 'creator': '', 'creationdate': '2017-05-01T10:37:35-07:00', 'moddate': '2017-05-01T10:37:35-07:00', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'source': 'data\\\\The_GALE_ENCYCLOPEDIA_of_MEDICINE_SECOND (1).pdf', 'total_pages': 759, 'page': 186, 'page_label': '187'}, page_content='Treatment\\nChoriocarcinomas are usually treated by surgical\\nremoval of the tumor and chemotherapy . Radiation is\\noccasionally used, particularly for tumors in the brain.\\nAlternative treatment\\nComplementary treatments can decrease stress,\\nreduce the side effects of cancer treatment, and help\\npatients feel more in control. For instance, some people\\nfind activities such as yoga, massage, music therapy ,\\nmeditation, prayer, or mild physical exercise helpful.\\nPrognosis'), Document(id='15a293a3-d98c-42d4-9b9c-ab962fe03821', metadata={'producer': 'GPL Ghostscript 9.10', 'creator': '', 'creationdate': '2017-05-01T10:37:35-07:00', 'moddate': '2017-05-01T10:37:35-07:00', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'source': 'data\\\\The_GALE_ENCYCLOPEDIA_of_MEDICINE_SECOND (1).pdf', 'total_pages': 759, 'page': 131, 'page_label': '132'}, page_content='py options may be explored.\\n• A secondary malignancy may develop from the one\\nbeing treated, and that second cancer may need addi-\\ntional chemotherapy or other treatment.\\nResources\\nBOOKS\\nDollinger, Malin, et al. Everyone’s Guide to Cancer Therapy:\\nHow Cancer is Diagnosed, Treated, and Managed Day to\\nDay.3rd ed. Kansas City: Andres & McMeel, 1998.\\nDrum, David. Making the Chemotherapy Decision. Los Ange-\\nles: Lowell House, 1996.\\nKEY TERMS\\nAdjuvant therapy —Treatment given after surgery')]\n"
     ]
    }
   ],
   "source": [
    "user_query=input(\"Write Query Here:\")\n",
    "response=qa_chain.invoke({'query':user_query})\n",
    "print(\"RESULT:\", response['result'])\n",
    "print(\"Source Document:\", response['source_documents'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
